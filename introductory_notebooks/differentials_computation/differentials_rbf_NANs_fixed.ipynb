{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a continuation of a [notebook](./differentials_rbf_NANs.ipynb) involving a case where the computation of differentials goes awry. In this notebook, this numerical error does not show up. We first show how differentials can be computed and then point out the implementation differences between this notebook and the previous.\n",
    "\n",
    "To this end, we make use of the `modules` folder, which contains the definitions of the classes and modules that will not case such numerical errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.append('../../modules/')\n",
    "from nn_rbf import RBF_Fix_All\n",
    "from notable_kernels import gaussian_kernel\n",
    "\n",
    "eps = 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7fbaeffd1490>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create an instance of the RBF interpolator equipped with the Gaussian kernel and $(0,0)$ as its center:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_rbf = RBF_Fix_All(centers=torch.zeros((1,2)), rbf_kernel=gaussian_kernel(eps))\n",
    "\n",
    "with torch.no_grad(): # just to avoid the constant factor\n",
    "    phi_rbf.output_layer.weight = torch.nn.Parameter(torch.Tensor([1]).reshape(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi_rbf_compare(x: torch.Tensor, y: torch.Tensor):\n",
    "    return torch.exp(-eps**2*(x ** 2 + y ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the same grid as in the previous notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000, 0.0000, 0.0000, 0.5000, 0.5000, 0.5000, 1.0000, 1.0000, 1.0000],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([0.0000, 0.5000, 1.0000, 0.0000, 0.5000, 1.0000, 0.0000, 0.5000, 1.0000],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "xy = torch.cartesian_prod(*[torch.linspace(0, 1, 3,requires_grad=True), \n",
    "                            torch.linspace(0, 1, 3, requires_grad=True)])\n",
    "x, y = xy[:, 0], xy[:, 1]\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We verify the correctness of our module with regards to the explicitly-defined function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phi_rbf_compare(x, y).reshape(-1,1) - phi_rbf(xy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now evaluate the derivatives along $x$ and $y$ on the points prescribed by the grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = phi_rbf(torch.cat((x.unsqueeze(1),y.unsqueeze(1)), dim=1))\n",
    "grad_x, grad_y = torch.autograd.grad(u, (x,y), \n",
    "                                     grad_outputs=torch.ones_like(u), \n",
    "                                     create_graph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compare the obtained results with the result of computing the differential by hand and then evaluating it, seeing that they are equal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<SubBackward0>)\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(grad_x - -2*eps**2*x*torch.exp(-eps**2*(x**2+y**2)))\n",
    "print(grad_y - -2*eps**2*y*torch.exp(-eps**2*(x**2+y**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do likewise, computing the second derivatives in $x$ and $y$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_xx = torch.autograd.grad(grad_x, x, grad_outputs=torch.ones_like(grad_x), create_graph=True, retain_graph=True)[0]\n",
    "u_yy = torch.autograd.grad(grad_y, y, grad_outputs=torch.ones_like(grad_y), create_graph=True, retain_graph=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compare the results yielded by PyTorch with regards to the derivative computed by hand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1921e-07, 0.0000e+00, 0.0000e+00,\n",
       "        3.5527e-15, 0.0000e+00, 0.0000e+00], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_xx - 2*eps**2*(-1+2*eps**2*x**2)*torch.exp(-eps**2*(x**2+y**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000e+00, 1.1921e-07, 3.5527e-15, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_yy - 2*eps**2*(-1+2*eps**2*y**2)*torch.exp(-eps**2*(x**2+y**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some numerical discrepancies. However, they are not very significant for most applications (being at the seventh decimal place in the worst case observed)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "What are we doing differently, so that NANs do not show up? We simply compare the definition provided in the previous notebook with the one featured in the `modules` folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### This implementation causes NANs when differentiating\n",
    "\n",
    "def compute_radii(x: torch.Tensor, centers: torch.Tensor) -> torch.Tensor:\n",
    "    x = x.unsqueeze(1)  # Shape: (batch_size, 1, d)\n",
    "    centers = centers.unsqueeze(0)  # Shape: (1, num_centers, d)\n",
    "    \n",
    "    squared_distances = torch.sum((x - centers)**2, dim=2)  # Shape: (batch_size, num_centers)\n",
    "    distances = torch.sqrt(squared_distances)\n",
    "    \n",
    "    return distances\n",
    "\n",
    "def gaussian_kernel(eps: float | torch.Tensor):\n",
    "    def fn(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "        radii = compute_radii(x, y)\n",
    "        return torch.exp(-(eps * radii) ** 2)\n",
    "    return fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### This implementation won't cause NANs when differentiating\n",
    "\n",
    "def compute_radii_squared(x: torch.Tensor, centers: torch.Tensor) -> torch.Tensor:\n",
    "    x = x.unsqueeze(1)  # Shape: (batch_size, 1, d)\n",
    "    centers = centers.unsqueeze(0)  # Shape: (1, num_centers, d)\n",
    "    \n",
    "    return torch.sum((x - centers)**2, dim=2)  # Shape: (batch_size, num_centers)\n",
    "\n",
    "\n",
    "def gaussian_kernel(eps: float | torch.Tensor):\n",
    "    def fn(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "        radii_sq = compute_radii_squared(x, y)\n",
    "        return torch.exp(-eps ** 2 * radii_sq)\n",
    "    return fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error message in the previous notebook details that NANs appear differentiating, concretely in the differential of the square root. This function is not differentiable at $x=0$. In the case the two inputs coincide for function `compute_radii()`, this error will show up.\n",
    "\n",
    "Because the Gaussian kernel uses the square of the radius anyway, instead of computing the radius we just do not take its square root, and account for this change in the kernel."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
